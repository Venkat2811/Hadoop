Big Data and Hadoop training course is designed to provide knowledge and skills to become a successful Hadoop Developer. In-depth knowledge of concepts such as Hadoop Distributed File System, Hadoop Cluster, Map-Reduce, Hbase Zookeeper etc. will be covered in the course.
Course Objectives
After the completion of the Hadoop Course at Edureka, you should be able to:
Master the concepts of Hadoop Distributed File System.
Understand Cluster Setup and Installation.
Understand MapReduce and Functional programming.
Implement HBase, MapReduce Integration, Advanced Usage and Advanced Indexing. 
Have a good understanding of ZooKeeper service and Sqoop.
Develop a working Hadoop Architecture.
Who should go for this course?
This course is designed for developers with some programming experience (preferably Java) who are looking forward to acquire a solid foundation of Hadoop Architecture. Existing knowledge of Hadoop is not required for this course.
Pre-requisites
Some of the prerequisites for learning Hadoop include hands-on experience in Java and good analytical skills to grasp and apply the concepts in Hadoop. 

Farhan Karmali,

Virtual Learning Environment Developer.
Islamic Online University
Connect at Linkedin
Edureka aptly named gives the students a "Eureka" Moment during the course. Learning is a world to explore and Edureka provides us with the navigation maps. I never for a minute felt that I am doing this course online away from the faculty and the staff.
Alexey Starykh,

R&D Engineer at IFAS of RWTH. Developer.
Aachen University, Germany 
Connect at Linkedin
Looks like they want us to think of nothing but learning.
I really appreciate this approach.
Thank you Edureka Team!
I had a nice and fruitful time.


Course Curriculum: Your 8 Week Learning Plan

Module 1
Introduction To Hadoop Distributed File Sytem (HDFS)
Learning Objectives - In this module, you will understand what is HDFS, why it is required for running Map-Reduce and how it differs from other distributed file systems. You will also get a basic idea how data gets fetched and written on HDFS.
Topics - Design of HDFS, HDFS Concepts, Command Line Interface, Hadoop File Systems, Java Interface, Data Flow (Anatomy of a File Read, Anatomy of a File Write, Coherency Model), Parallel Copying with DISTCP, Hadoop Archives.



Module 2
Setting Up Hadoop Cluster
Learning Objectives -After this module, you will get a clear understanding of How to setup Hadoop Cluster and about different configuration files which need to be edited for Cluster setup.
Topics -Cluster Specification, Cluster Setup and Installation, SSH Configuration, Hadoop Configuration (Configuration Management, Environment Settings, Important Hadoop Daemon Properties, Hadoop Daemon Addresses and Ports, Other Hadoop Properties, User Account Creation), Security, Benchmarking a Hadoop Cluster.



Module 3
Understanding - Map-Reduce Basics and Map-Reduce Types and Formats
Learning Objectives - After this module, you will get an idea of how Map-Reduce framework works and why Map-Reduce is tightly coupled with HDFS. Also, you will learn what the different types of Input and Output formats are and why they are required.
Topics -Hadoop Data Types, Functional Programming Roots, Imperative vs Functional Programming, Concurrency and lock free data structure, Functional - Concept of Mappers, Functional - Concept of Reducers, The Execution Framework (Scheduling, Data/Code co-location, Synchronization, Error and Fault Handling), Functional - Concept of Partitioners, Functional - Concept of Combiners, Distributed File System, Hadoop Cluster Architecture, MapReduce Types, Input Formats (Input Splits and Records, Text Input, Binary Input, Multiple Inputs, Database Input and Output), OutPut Formats (TextOutput, BinaryOutPut, Multiple Outputs, Databaseoutput).



Module 4
PIG
Learning Objectives -In this module you will learn What is Pig, In Which type of use case we can use Pig, How Pig is tightly coupled with Map-Reduce, along with an example.
Topics -Installing and Running Pig, Grunt, Pig's Data Model, Pig Latin, Developing & Testing Pig Latin Scripts, Making Pig Fly, Writing Evaluation, Filter, Load & Store Functions, Pig and Other Members of the Hadoop Community.



Module 5
HIVE
Learning Objectives -This module will provide you with a clear understanding of What is HIVE, How you can load data into HIVE and query data from Hive and so on.
Topics -Installing Hive, Running Hive (Configuring Hive, Hive Services, MetaStore), Comparison with Traditional Database (Schema on Read Versus Schema on Write, Updates, Transactions and Indexes), HiveQL (Data Types, Operators and Functions), Tables (Managed Tables and External Tables, Partitions and Buckets, Storage Formats, Importing Data, Altering Tables, Dropping Tables), Querying Data (Sorting And Aggregating, Map Reduce Scripts, Joings & Subqueries & Views, Map and Reduce site Join to optimize Query), User Defined Functions, Appending Data into existing Hive Table, Custom Map/Reduce in Hive.



Module 6
HBASE
Learning Objectives -In this module you will acquire in-depth knowledge of What is Hbase, How you can load data into Hbase and query data from Hbase using client and so on.
Topics -Introduction, Installation, Client API - Basics, Client API - Advanced Features, Client API - Administrative Features, Available Client, Architecture, MapReduce Integration, Advanced Usage, Advance Indexing.



Module 7
ZOOKEEPER
Learning Objectives -At the end of this module, you will learn about What is a Zookeeper, How it helps in monitoring a cluster and Why Hbase uses Zookeeper.
Topics -The Zookeeper Service (Data Modal, Operations, Implementation, Consistancy, Sessions, States), Building Applications with Zookeeper (Zookeeper in Production).



Module 8
SQOOP
Learning Objectives -After this last module you will get to know What is Sqoop, How you can do Import and export In/from Hdfs and What is the Internal Architecture of Sqoop.
Topics -Database Imports, Working with Imported Data, Importing Large Objects, Performing Exports, Exports - A Deeper Look.
Buy Course
Why Learn Hadoop?
BiG Data! A Worldwide Problem?

According to Wikipedia, “Big data is collection of data sets so large and complex that it becomes difficult to process using on-hand database management tools or traditional data processing applications.” In simpler terms, Big Data is a term given to large volumes of data that organizations store and process. However, It is becoming very difficult for companies to store, retrieve and process the ever-increasing data. If any company gets hold on managing its data well, nothing can stop it from becoming the next BIG success!

The problem lies in the use of traditional systems to store enormous data. Though these systems were a success a few years ago, with increasing amount and complexity of data, these are soon becoming obsolete. The good news is - Hadoop, which is not less than a panacea for all those companies working with BIG DATA in a variety of applications and has become an integral part for storing, handling, evaluating and retrieving hundreds of terabytes, and even petabytes of data.

Apache Hadoop! A Solution for Big Data! 

Hadoop is an open source software framework that supports data-intensive distributed applications. Hadoop is licensed under the Apache v2 license. It is therefore generally known as Apache Hadoop. Hadoop has been developed, based on a paper originally written by Google on MapReduce system and applies concepts of functional programming. Hadoop is written in the Java programming language and is the highest-level Apache project being constructed and used by a global community of contributors. Hadoop was developed by Doug Cutting and Michael J. Cafarella. And just don’t overlook the charming yellow elephant you see, which is basically named after Doug’s son’s toy elephant!

Some of the top companies using Hadoop: 

The importance of Hadoop is evident from the fact that there are many global MNCs that are using Hadoop and consider it as an integral part of their functioning, such as companies like Yahoo and Facebook! On February 19, 2008, Yahoo! Inc. established the world's largest Hadoop production application. The Yahoo! Search Webmap is a Hadoop application that runs on over 10,000 core Linux cluster and generates data that is now widely used in every Yahoo! Web search query.

Facebook, a $5.1 billion company has over 1 billion active users in 2012, according to Wikipedia. Storing and managing data of such magnitude could have been a problem, even for a company like Facebook. But thanks to Apache Hadoop! Facebook uses Hadoop to keep track of each and every profile it has on it, as well as all the data related to them like their images, posts, comments, videos, etc.

Opportunities for Hadoopers!

Opportunities for Hadoopers are infinite - from a Hadoop Developer, to a Hadoop Tester or a Hadoop Architect, and so on. If cracking and managing BIG Data is your passion in life, then think no more and Join Edureka’s Hadoop Online course and carve a niche for yourself! Happy Hadooping!